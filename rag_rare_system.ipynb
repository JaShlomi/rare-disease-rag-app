{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-RARE System: Advanced Retrieval-Augmented Generation\n",
    "\n",
    "This notebook implements an advanced Retrieval-Augmented Generation (RAG) system, designed to provide accurate and contextually relevant responses by combining the power of Large Language Models (LLMs) with external knowledge retrieval. The system is built to handle various document types, perform intelligent chunking, leverage powerful embeddings, and integrate with a vector store for efficient semantic search.\n",
    "\n",
    "## Key Features:\n",
    "\n",
    "1.  **Multi-Document Support**: Capable of processing diverse document formats.\n",
    "2.  **Context-Aware Chunking**: Utilizes `RecursiveCharacterTextSplitter` to maintain semantic coherence within document chunks.\n",
    "3.  **Robust Embeddings**: Employs OpenAI's `text-embedding-ada-002` for high-quality vector representations.\n",
    "4.  **Efficient Vector Store**: Uses `FAISS` (Facebook AI Similarity Search) for fast and scalable similarity search.\n",
    "5.  **Advanced Retrieval**: Implements `MultiQueryRetriever` and `ContextualCompressionRetriever` for improved query understanding and relevant document selection.\n",
    "6.  **Powerful LLM Integration**: Leverages `ChatOpenAI` for generating coherent and contextually rich responses.\n",
    "7.  **Conversational Memory**: Integrates `ConversationBufferMemory` to maintain conversation history, enabling follow-up questions.\n",
    "8.  **Chaining for Cohesion**: Utilizes `ConversationalRetrievalChain` to seamlessly integrate retrieval and generation components.\n",
    "\n",
    "## Setup and Installation\n",
    "\n",
    "Before running the notebook, ensure you have the necessary libraries installed and your environment variables configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install -qU langchain langchain-openai pypdf faiss-cpu tiktoken transformers langchain_community\n",
    "\n",
    "# Import required modules\n",
    "import os\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Set your OpenAI API key. **Crucially, never hardcode your API keys directly in publicly shared notebooks.** It's best practice to load them from environment variables.\n",
    "\n",
    "To do this:\n",
    "1.  Create a `.env` file in your project directory (if not already present).\n",
    "2.  Add your API key to it: `OPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\"`\n",
    "3.  Load the environment variables at the start of your script or notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key from environment variables\n",
    "# Make sure to set OPENAI_API_KEY in your environment before running this.\n",
    "# For example: os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# It's highly recommended to load this from a .env file using python-dotenv\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\" # Replace with your actual key or load from env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Loading and Preprocessing\n",
    "\n",
    "This section handles loading documents. You can load various file types. For demonstration, we'll use a placeholder for document paths. Replace `'path/to/your/document.pdf'` or `'path/to/your/textfile.txt'` with your actual document paths.\n",
    "\n",
    "We use `PyPDFLoader` for PDF files and `TextLoader` for text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example document loading (replace with your actual document paths)\n",
    "\n",
    "# For a PDF document:\n",
    "# loader = PyPDFLoader(\"path/to/your/document.pdf\") \n",
    "# documents = loader.load()\n",
    "\n",
    "# For a text document:\n",
    "# loader = TextLoader(\"path/to/your/textfile.txt\")\n",
    "# documents = loader.load()\n",
    "\n",
    "# Placeholder: Creating dummy documents for demonstration if no files are loaded\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "dummy_content = [\n",
    "    \"The quick brown fox jumps over the lazy dog. This is a common English pangram used for testing typewriters and computer keyboards.\",\n",
    "    \"Artificial intelligence is rapidly advancing, with new breakthroughs in machine learning and neural networks. It has applications in various fields.\",\n",
    "    \"Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. It turns data scripts into shareable web apps in minutes.\",\n",
    "    \"Retrieval-Augmented Generation (RAG) combines information retrieval with text generation. It allows language models to access external knowledge bases, improving factual accuracy and reducing hallucinations.\",\n",
    "    \"FAISS, developed by Facebook AI, is a library for efficient similarity search and clustering of dense vectors. It is widely used for building vector databases.\"\n",
    "]\n",
    "\n",
    "documents = [Document(page_content=content) for content in dummy_content]\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Splitting (Chunking)\n",
    "\n",
    "Documents are often too large to fit into an LLM's context window directly. We split them into smaller, semantically coherent chunks using `RecursiveCharacterTextSplitter`. This method attempts to split by paragraphs, then sentences, then words, ensuring that chunks try to maintain context.\n",
    "\n",
    "- `chunk_size`: The maximum size of each text chunk.\n",
    "- `chunk_overlap`: The number of characters to overlap between consecutive chunks. This helps maintain context across chunk boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# Split the loaded documents into chunks\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split into {len(texts)} chunks.\")\n",
    "# Optional: print first chunk to verify\n",
    "# print(texts[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embeddings and Vector Store Creation\n",
    "\n",
    "This step converts our text chunks into numerical vector representations (embeddings) and stores them in a vector database for efficient similarity search.\n",
    "\n",
    "- **Embeddings**: We use `OpenAIEmbeddings` with the `text-embedding-ada-002` model, known for its high quality.\n",
    "- **Vector Store**: `FAISS` is chosen for its in-memory performance and suitability for local development. For production, you might consider persistent or cloud-based vector stores.\n",
    "\n",
    "The embeddings model converts text into a dense vector, where semantically similar texts are closer in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI embeddings model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Create a FAISS vector store from the document chunks and embeddings\n",
    "vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "print(\"FAISS vector store created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize the Large Language Model (LLM)\n",
    "\n",
    "We initialize the `ChatOpenAI` model, which will be responsible for generating answers based on the retrieved context.\n",
    "\n",
    "- `model_name`: Specifies the OpenAI model to use (e.g., `gpt-3.5-turbo`, `gpt-4`).\n",
    "- `temperature`: Controls the randomness of the output. Lower values (e.g., 0) make the output more deterministic and factual, while higher values make it more creative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ChatOpenAI model\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "print(\"LLM initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Retrieval Setup (MultiQuery and Contextual Compression)\n",
    "\n",
    "To improve retrieval quality, we employ two advanced techniques:\n",
    "\n",
    "### a. MultiQueryRetriever\n",
    "Generates multiple distinct queries from a single user query. This helps to explore different facets of the user's intent and potentially retrieve more relevant documents, especially for ambiguous or broad questions.\n",
    "\n",
    "### b. ContextualCompressionRetriever\n",
    "This retriever compresses the retrieved documents to only include the most relevant parts. It uses an LLM (`LLMChainExtractor`) to identify and extract the passages most pertinent to the query, reducing noise and improving the LLM's focus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MultiQueryRetriever Setup ---\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database.\n",
    "By generating multiple perspectives on the user's original query, your goal is to help the user retrieve a broader set of relevant documents.\n",
    "The original question is: {question}\n",
    "\"\"\"\n",
    "prompt_perspectives = PromptTemplate(input_variables=[\"question\"], template=template)\n",
    "\n",
    "# Define the MultiQueryRetriever\n",
    "# Note: The vectorstore.as_retriever() creates a basic retriever from the FAISS index\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ")\n",
    "\n",
    "print(\"MultiQueryRetriever initialized.\")\n",
    "\n",
    "# --- ContextualCompressionRetriever Setup ---\n",
    "# This uses an LLM to extract only the relevant parts of the retrieved documents\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever_from_llm # Using MultiQueryRetriever as the base\n",
    ")\n",
    "\n",
    "print(\"ContextualCompressionRetriever initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conversational Memory\n",
    "\n",
    "To enable a multi-turn conversation, we use `ConversationBufferMemory`. This stores previous messages (both user inputs and AI responses) and injects them into the context for subsequent queries, allowing the LLM to understand and respond to follow-up questions contextually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True,\n",
    "    output_key='answer'\n",
    ")\n",
    "\n",
    "print(\"Conversation memory initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conversational Retrieval Chain\n",
    "\n",
    "The `ConversationalRetrievalChain` is the core of our RAG system. It orchestrates the entire process:\n",
    "\n",
    "1.  **Retrieval**: Takes the user's query and conversation history, generates relevant queries (via `MultiQueryRetriever`), fetches documents (via `FAISS`), and compresses them (via `ContextualCompressionRetriever`).\n",
    "2.  **Context Construction**: Combines the retrieved documents with the conversation history.\n",
    "3.  **Generation**: Passes this combined context to the LLM (`ChatOpenAI`) to generate a coherent and informed answer.\n",
    "\n",
    "- `llm`: The language model used for generation.\n",
    "- `retriever`: The retriever responsible for fetching relevant documents. Here we use our enhanced `compression_retriever`.\n",
    "- `memory`: The component that stores and manages conversation history.\n",
    "- `return_source_documents`: If `True`, the chain will return the source documents used to generate the answer, which is useful for debugging and transparency.\n",
    "- `return_generated_question`: If `True`, the chain will return the question it generated to query the retriever based on the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the conversational retrieval chain\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=compression_retriever, # Use the advanced compression retriever\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    return_generated_question=True\n",
    ")\n",
    "\n",
    "print(\"ConversationalRetrievalChain initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interacting with the RAG System\n",
    "\n",
    "Now you can interact with your RAG system by asking questions. The `qa` object will process your query, retrieve relevant information, and generate a response. The `chat_history` will maintain context across turns.\n",
    "\n",
    "**Example Usage:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example questions\n",
    "query1 = \"What is RAG?\"\n",
    "result1 = qa.invoke({\"question\": query1})\n",
    "print(f\"Question 1: {query1}\")\n",
    "print(f\"Answer 1: {result1['answer']}\")\n",
    "print(\"--- Source Documents for Q1 ---\")\n",
    "for i, doc in enumerate(result1['source_documents']):\n",
    "    print(f\"Document {i+1}:\\n{doc.page_content[:200]}...\") # Print first 200 chars\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "query2 = \"How does it improve LLMs?\"\n",
    "result2 = qa.invoke({\"question\": query2})\n",
    "print(f\"Question 2: {query2}\")\n",
    "print(f\"Answer 2: {result2['answer']}\")\n",
    "print(\"--- Source Documents for Q2 ---\")\n",
    "for i, doc in enumerate(result2['source_documents']):\n",
    "    print(f\"Document {i+1}:\\n{doc.page_content[:200]}...\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "query3 = \"Can you tell me about Streamlit and FAISS?\"\n",
    "result3 = qa.invoke({\"question\": query3})\n",
    "print(f\"Question 3: {query3}\")\n",
    "print(f\"Answer 3: {result3['answer']}\")\n",
    "print(\"--- Source Documents for Q3 ---\")\n",
    "for i, doc in enumerate(result3['source_documents']):\n",
    "    print(f\"Document {i+1}:\\n{doc.page_content[:200]}...\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
